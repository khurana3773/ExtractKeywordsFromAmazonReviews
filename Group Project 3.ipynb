{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Imports.\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining most used Data structures and inputs\n",
    "Features = {'screen':[],'sound':[],'camera':[]}\n",
    "\n",
    "data = pd.read_json('Jsons2/B004MN00C4.json', typ='series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extracting data from input to data structures\n",
    "#A Typical Json file of amazon reviews is like:\n",
    "# {\n",
    "#     \"Reviews\":[\n",
    "#     {\n",
    "#         \"Content\":\"Actual Review 1\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"Content\":\"Actual Review 2\"\n",
    "#     },    \n",
    "#     .\n",
    "#     .    \n",
    "#     ],\n",
    "#     .\n",
    "#     .\n",
    "# }\n",
    "\n",
    "for review in data['Reviews']:   \n",
    "    reviewValue = review.get('Content')\n",
    "    if reviewValue is None:\n",
    "        continue\n",
    "    sent_tokenize_list = sent_tokenize(reviewValue) #Tokenize long reviews into individual strings\n",
    "    for eachReview in sent_tokenize_list:\n",
    "        for feature in Features:\n",
    "            if feature in eachReview:\n",
    "                Features[feature].append(eachReview)\n",
    "\n",
    "#Long reviews were normalized, as one long review should not change the outcome of the whole sentiment. \n",
    "#The dataset didn`t contain info about the reviews which were most useful.\n",
    "#Hence, assumption made, that longer the review, more helpful it was.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#A preprocessed dataset of pros and cons phrases was taken from NLKT corpus.\n",
    "\n",
    "#read all statements\n",
    "short_pos = open(\"Jsons2/pros.txt\",\"r\").read()\n",
    "short_neg = open(\"Jsons2/cons.txt\",\"r\").read()\n",
    "stopwords_txt = open(\"Jsons2/stopwords.txt\",\"r\").read()\n",
    "\n",
    "documents = []\n",
    "documentspos = []\n",
    "documentsneg = []\n",
    "stopwords = []\n",
    "\n",
    "#labelling statements from pros as pos\n",
    "for r in short_pos.split('\\n'):\n",
    "    documentspos.append( (r, \"pos\") )\n",
    "\n",
    "#labelling statements from cons as neg\n",
    "for r in short_neg.split('\\n'):\n",
    "    documentsneg.append( (r, \"neg\") )\n",
    "    \n",
    "#Collecting all stopwords in a datastructure\n",
    "for r in stopwords_txt.split('\\n'):\n",
    "    stopwords.append(r)\n",
    "\n",
    "#tokenize all words and add to get most frequent words\n",
    "all_words = []\n",
    "\n",
    "short_pos_words = word_tokenize(short_pos)\n",
    "short_neg_words = word_tokenize(short_neg)\n",
    "\n",
    "for w in short_pos_words:\n",
    "    if w not in stopwords:\n",
    "        all_words.append(w.lower())\n",
    "\n",
    "for w in short_neg_words:\n",
    "    if w not in stopwords:\n",
    "        all_words.append(w.lower())\n",
    "\n",
    "#From all useful words, we create a freqdistribution        \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print((all_words.items()))\n",
    "\n",
    "#a total of 12633 words were present, and we considered various values, 10000, 8000, and 5000.\n",
    "#finally used 7000 to train our model.\n",
    "#we removed stop words, punctuations from these words.\n",
    "word_features = list(all_words.keys())[:7000]\n",
    "\n",
    "\n",
    "\n",
    "#This is the main function which extracts most frequent words from a passed string.\n",
    "#the words are also lemmatized.\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "#This function is used to finally train the machine learning algorithms.\n",
    "featuresetspos = [(find_features(rev), category) for (rev, category) in documentspos]\n",
    "featuresetsneg = [(find_features(rev), category) for (rev, category) in documentsneg]\n",
    "\n",
    "\n",
    "poscutoff = len(featuresetspos)*3/4\n",
    "negcutoff = len(featuresetsneg)*3/4\n",
    "\n",
    "random.shuffle(featuresetspos)\n",
    "random.shuffle(featuresetsneg)\n",
    "\n",
    "trainfeats = featuresetsneg[:negcutoff] + featuresetspos[:poscutoff]\n",
    "testfeats = featuresetsneg[negcutoff:] + featuresetspos[poscutoff:]\n",
    "\n",
    "\n",
    "random.shuffle(trainfeats)\n",
    "random.shuffle(testfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train Naive Bayes\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11471\n"
     ]
    }
   ],
   "source": [
    "print(len(testfeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e269db2b515f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrefsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mobserved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtestsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Student/anaconda2/lib/python2.7/site-packages/nltk/classify/naivebayes.pyc\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, featureset)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Student/anaconda2/lib/python2.7/site-packages/nltk/classify/naivebayes.pyc\u001b[0m in \u001b[0;36mprob_classify\u001b[0;34m(self, featureset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_probdist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mfeature_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_probdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mlogprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfeature_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;31m# nb: This case will never come up if the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Student/anaconda2/lib/python2.7/site-packages/nltk/probability.pyc\u001b[0m in \u001b[0;36mlogprob\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \"\"\"\n\u001b[1;32m    478\u001b[0m         \u001b[0;31m# Default definition, in terms of prob()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_NINF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Student/anaconda2/lib/python2.7/site-packages/nltk/probability.pyc\u001b[0m in \u001b[0;36mprob\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_freqdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_divisor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import nltk.classify.util, nltk.metrics\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(testfeats):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "    \n",
    "#calculate accuracy of Naive Bayes Classifier\n",
    "accuracy = nltk.classify.util.accuracy(classifier, testfeats)\n",
    "# pos_precision = nltk.metrics.precision(refsets['pos'], testsets['pos'])\n",
    "# pos_recall = nltk.metrics.recall(refsets['pos'], testsets['pos'])\n",
    "# pos_fmeasure = nltk.metrics.f_measure(refsets['pos'], testsets['pos'])\n",
    "# neg_precision = nltk.metrics.precision(refsets['neg'], testsets['neg'])\n",
    "# neg_recall = nltk.metrics.recall(refsets['neg'], testsets['neg'])\n",
    "# neg_fmeasure =  nltk.metrics.f_measure(refsets['neg'], testsets['neg'])\n",
    "        \n",
    "print('')\n",
    "print('---------------------------------------')\n",
    "print(\"Single Fold for Naive Bayes \")\n",
    "print('---------------------------------------')\n",
    "print('accuracy:', accuracy)\n",
    "# print('precision', (pos_precision + neg_precision) / 2)\n",
    "# print('recall', (pos_recall + neg_recall) / 2)\n",
    "# print('f-measure', (pos_fmeasure + neg_fmeasure) / 2)  \n",
    "                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### cross validation\n",
    "\n",
    "trainfeats = featuresetsneg + featuresetspos    \n",
    "  \n",
    "random.shuffle(trainfeats)\n",
    "\n",
    "n = 5\n",
    "subset_size = len(trainfeats) / n\n",
    "    accuracy = []\n",
    "    pos_precision = []\n",
    "    pos_recall = []\n",
    "    neg_precision = []\n",
    "    neg_recall = []\n",
    "    pos_fmeasure = []\n",
    "    neg_fmeasure = []\n",
    "    cv_count = 1\n",
    "    for i in range(n):        \n",
    "        testing_this_round = trainfeats[i*subset_size:][:subset_size]\n",
    "        training_this_round = trainfeats[:i*subset_size] + trainfeats[(i+1)*subset_size:]\n",
    "\n",
    "        cv_accuracy = nltk.classify.util.accuracy(classifier, testing_this_round)\n",
    "        cv_pos_precision = nltk.metrics.precision(refsets['pos'], testsets['pos'])\n",
    "        cv_pos_recall = nltk.metrics.recall(refsets['pos'], testsets['pos'])\n",
    "        cv_pos_fmeasure = nltk.metrics.f_measure(refsets['pos'], testsets['pos'])\n",
    "        cv_neg_precision = nltk.metrics.precision(refsets['neg'], testsets['neg'])\n",
    "        cv_neg_recall = nltk.metrics.recall(refsets['neg'], testsets['neg'])\n",
    "        cv_neg_fmeasure =  nltk.metrics.f_measure(refsets['neg'], testsets['neg'])\n",
    "\n",
    "        accuracy.append(cv_accuracy)\n",
    "        pos_precision.append(cv_pos_precision)\n",
    "        pos_recall.append(cv_pos_recall)\n",
    "        neg_precision.append(cv_neg_precision)\n",
    "        neg_recall.append(cv_neg_recall)\n",
    "        pos_fmeasure.append(cv_pos_fmeasure)\n",
    "        neg_fmeasure.append(cv_neg_fmeasure)\n",
    "\n",
    "    cv_count += 1\n",
    "                \n",
    "        print('---------------------------------------')\n",
    "        print('N-FOLD CROSS VALIDATION RESULT Naive Bayes)')\n",
    "        print('---------------------------------------')\n",
    "        print('accuracy:', sum(accuracy) / n)\n",
    "        print('precision', (sum(pos_precision)/n + sum(neg_precision)/n) / 2)\n",
    "        print('recall', (sum(pos_recall)/n + sum(neg_recall)/n) / 2)\n",
    "        print('f-measure', (sum(pos_fmeasure)/n + sum(neg_fmeasure)/n) / 2)\n",
    "        print('')\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('(+) Good ', 'sound', '--- pos :', 40, 'neg :', 8)\n",
      "()\n",
      "('(+) Good ', 'screen', '--- pos :', 472, 'neg :', 66)\n",
      "()\n",
      "('(+) Good ', 'camera', '--- pos :', 5138, 'neg :', 800)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "for feature,featureReviews in Features.items():\n",
    "    pos =0\n",
    "    neg =0\n",
    "    for sentence in featureReviews:\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        if ss['pos'] >= ss['neg']:\n",
    "            pos = pos + 1\n",
    "        else:\n",
    "            neg = neg + 1\n",
    "    if pos > neg:\n",
    "        print(\"(+) Good \",feature,\"--- pos :\",pos,\"neg :\",neg)\n",
    "    else:\n",
    "        print(\"(-) Bad  \",feature,\"--- pos :\",pos,\"neg :\",neg)\n",
    "    print() \n",
    "            \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Student/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:56: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('(-) Bad  ', 'sound', '--- pos :', 10, 'neg :', 38)\n",
      "()\n",
      "('(-) Bad  ', 'screen', '--- pos :', 160, 'neg :', 378)\n",
      "()\n",
      "('(-) Bad  ', 'camera', '--- pos :', 1292, 'neg :', 4646)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "classifier_f = open(\"NaiveBayesClassifier.pickle\", \"rb\")\n",
    "NaiveBayesClassifier = pickle.load(classifier_f)\n",
    "classifier_f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for feature,featureReviews in Features.items():\n",
    "    pos =0\n",
    "    neg =0\n",
    "    for sentence in featureReviews:\n",
    "        ss = NaiveBayesClassifier.classify(find_features(sentence))\n",
    "        if 'pos' == ss:\n",
    "            pos = pos + 1\n",
    "        else:\n",
    "            neg = neg + 1\n",
    "    if pos > neg:\n",
    "        print(\"(+) Good \",feature,\"--- pos :\",pos,\"neg :\",neg)\n",
    "    else:\n",
    "        print(\"(-) Bad  \",feature,\"--- pos :\",pos,\"neg :\",neg)\n",
    "    print() \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " NaiveBayesClassifier.classify(find_features(\"This is not a great awesome phone\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
