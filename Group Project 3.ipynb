{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Imports.\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining most used Data structures and inputs\n",
    "Features = {'screen':[],'sound':[],'camera':[]}\n",
    "\n",
    "data = pd.read_json('Jsons2/B00B93KG1A.json', typ='series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting data from input to data structures\n",
    "#A Typical Json file of amazon reviews is like:\n",
    "# {\n",
    "#     \"Reviews\":[\n",
    "#     {\n",
    "#         \"Content\":\"Actual Review 1\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"Content\":\"Actual Review 2\"\n",
    "#     },    \n",
    "#     .\n",
    "#     .    \n",
    "#     ],\n",
    "#     .\n",
    "#     .\n",
    "# }\n",
    "\n",
    "for review in data['Reviews']:   \n",
    "    reviewValue = review.get('Content')\n",
    "    if reviewValue is None:\n",
    "        continue\n",
    "    sent_tokenize_list = sent_tokenize(reviewValue) #Tokenize long reviews into individual strings\n",
    "    for eachReview in sent_tokenize_list:\n",
    "        for feature in Features:\n",
    "            if feature in eachReview:\n",
    "                Features[feature].append(eachReview)\n",
    "\n",
    "#Long reviews were normalized, as one long review should not change the outcome of the whole sentiment. \n",
    "#The dataset didn`t contain info about the reviews which were most useful.\n",
    "#Hence, assumption made, that longer the review, more helpful it was.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#A preprocessed dataset of pros and cons phrases was taken from NLKT corpus.\n",
    "\n",
    "#read all statements\n",
    "short_pos = open(\"Jsons2/pros.txt\",\"r\").read()\n",
    "short_neg = open(\"Jsons2/cons.txt\",\"r\").read()\n",
    "stopwords_txt = open(\"Jsons2/stopwords.txt\",\"r\").read()\n",
    "\n",
    "documents = []\n",
    "stopwords = []\n",
    "\n",
    "#labelling statements from pros as pos\n",
    "for r in short_pos.split('\\n'):\n",
    "    documents.append( (r, \"pos\") )\n",
    "\n",
    "#labelling statements from cons as neg\n",
    "for r in short_neg.split('\\n'):\n",
    "    documents.append( (r, \"neg\") )\n",
    "    \n",
    "#Collecting all stopwords in a datastructure\n",
    "for r in stopwords_txt.split('\\n'):\n",
    "    stopwords.append(r)\n",
    "\n",
    "#tokenize all words and add to get most frequent words\n",
    "all_words = []\n",
    "\n",
    "short_pos_words = word_tokenize(short_pos)\n",
    "short_neg_words = word_tokenize(short_neg)\n",
    "\n",
    "for w in short_pos_words:\n",
    "    if w not in stopwords:\n",
    "        all_words.append(w.lower())\n",
    "\n",
    "for w in short_neg_words:\n",
    "    if w not in stopwords:\n",
    "        all_words.append(w.lower())\n",
    "\n",
    "#From all useful words, we create a freqdistribution        \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print((all_words.items()))\n",
    "\n",
    "#a total of 12633 words were present, and we considered various values, 10000, 8000, and 5000.\n",
    "#finally used 7000 to train our model.\n",
    "#we removed stop words, punctuations from these words.\n",
    "word_features = list(all_words.keys())[:7000]\n",
    "\n",
    "\n",
    "\n",
    "#This is the main function which extracts most frequent words from a passed string.\n",
    "#the words are also lemmatized.\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "#This function is used to finally train the machine learning algorithms.\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "#Shuffling to disallign pros and cons together\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "\n",
    "#dividing total 44356 into 2 parts for training and testing\n",
    "#35000 for training\n",
    "#9356 for testing\n",
    "trainingData = featuresets[:35000]\n",
    "testData = featuresets[35000:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "for feature,featureReviews in Features.items():\n",
    "    pos =0\n",
    "    neg =0\n",
    "    for sentence in featureReviews:\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        if ss['pos'] >= ss['neg']:\n",
    "            pos = pos + 1\n",
    "        else:\n",
    "            neg = neg + 1\n",
    "    if pos > neg:\n",
    "        print(\"(+) Good \",feature,\"--- pos :\",pos,\"neg :\",neg)\n",
    "    else:\n",
    "        print(\"(-) Bad  \",feature,\"--- pos :\",pos,\"neg :\",neg)\n",
    "    print() \n",
    "            \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
