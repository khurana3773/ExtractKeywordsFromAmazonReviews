{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "  \n",
    "hotel_rev = ['I may not be perfect, but atleast I am not you.']\n",
    "  \n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in hotel_rev:\n",
    "    print(sentence)\n",
    "    \n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    for k in ss:\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"Kilometer\"\n",
    "print(s.lower())\n",
    "\n",
    "\n",
    "# score = vader.polarity_scores(text)\n",
    "# return 1 if score['pos'] > score['neg'] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testDict = {\"a\":123}\n",
    "\n",
    "for key, value in testDict.items():\n",
    "    if 'b' in testDict:\n",
    "        print(\"Hi\")\n",
    "    else:\n",
    "        print(\"bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shortHandWords(sortedDict):\n",
    "    retDict = {\"test\":-1}\n",
    "    for key, value in sortedDict.items():\n",
    "        print(key.lower())\n",
    "         \n",
    "       \n",
    "    return retDict;\n",
    "\n",
    "\n",
    "retDict = shortHandWords(testDict)\n",
    "print(retAns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "nouns = {x.name().split('.', 1)[0] for x in wn.all_synsets('v')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = 'lens'\n",
    "\n",
    "if key not in nouns:\n",
    "    print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "#Step 1 – Training data\n",
    "train = [(\"Great place to be when you are in Bangalore.\", \"+\"),\n",
    "  (\"The place was being renovated when I visited so the seating was limited.\", \"neg\"),\n",
    "  (\"Loved the ambience, loved the food\", \"pos\"),\n",
    "  (\"The food is delicious but not over the top.\", \"neg\"),\n",
    "  (\"Service - Little slow, probably because too many people.\", \"neg\"),\n",
    "  (\"The place is not easy to locate\", \"neg\"),\n",
    "  (\"Mushroom fried rice was spicy\", \"neg\"),\n",
    "  (\"Mushroom fried rice was hot\", \"neg\"),\n",
    "  (\"Mushroom fried rice was hot and spicy\", \"+\")]\n",
    "\n",
    "\n",
    "\n",
    "train.append((\"Mushroom fried rice was hot and spicy\", \"+\"))\n",
    "\n",
    "print(type(train))\n",
    "\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "#scan for those keywords in the reviews\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "# Step 1 – Training data\n",
    "# train = [(\"not so good\", \"-\"),\n",
    "#   (\"could have been better\", \"-\"),\n",
    "#   (\"bad and pathetic\", \"-\"),\n",
    "#   (\"excellent performance\", \"+\"),\n",
    "#   (\"not that accurate\", \"-\"),\n",
    "#   (\"isn`t that bad\", \"-\"),\n",
    "#   (\"screen is bright\", \"+\"),\n",
    "#   (\"beautiful and solid\", \"+\"),\n",
    "#   (\"amazing and sturdy\", \"+\")]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# path_to_json = 'Jsons2/'\n",
    "# json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "# i = 0\n",
    "\n",
    "# #logic for iteration.\n",
    "# for file in json_files:    \n",
    "#     data = pd.read_json(path_to_json+file, typ='series')\n",
    "#     print(file,\"  \",len(data['Reviews']) )\n",
    "#     for review in data['Reviews']:   \n",
    "#             i = i +1\n",
    "#             print(i)\n",
    "#             reviewValue = review.get('Content')\n",
    "#             if reviewValue is None:\n",
    "#                 continue\n",
    "#             if float(review.get('Overall'))>=3:  \n",
    "#                 train.append((review.get('Content'),\"pos\"))\n",
    "#             if float(review.get('Overall'))<3:\n",
    "#                 train.append((review.get('Content'),\"neg\"))\n",
    "            \n",
    "            \n",
    "#Step 2\n",
    "dictionary = set(word.lower() for passage in train for word in word_tokenize(passage[0]))\n",
    "  \n",
    "#Step 3\n",
    "t = [({word: (word in word_tokenize(x[0])) for word in dictionary}, x[1]) for x in train]\n",
    "  \n",
    "# Step 4 – the classifier is trained with sample data\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "  \n",
    "test_data = \"bAD Camera\"\n",
    "test_data_features = {word.lower(): (word in word_tokenize(test_data.lower())) for word in dictionary}\n",
    "  \n",
    "print(classifier.classify(test_data_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import xml.etree.ElementTree\n",
    "e = xml.etree.ElementTree.parse('thefile.xml').getroot()\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Easy', 'to', 'use', ',', 'economical', '!'], ['Digital', 'is', 'where', 'it', \"'\", 's', 'at', '...', 'down', 'with', 'developing', 'film', '!'], ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import pros_cons\n",
    "pros_cons.(categories='Pros')\n",
    "\n",
    "# import xml.etree.ElementTree\n",
    "# e = xml.etree.ElementTree.parse('pros.xml').getroot()\n",
    "\n",
    "# for pros in e.findall('type'):\n",
    "#     print(atype.get('foobar'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
